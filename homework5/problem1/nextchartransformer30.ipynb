{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n",
      "===============================================================================================\n",
      "Layer (type:depth-idx)                        Output Shape              Param #\n",
      "===============================================================================================\n",
      "CharTransformer                               [1889, 30, 47]            --\n",
      "├─Embedding: 1-1                              [1889, 30, 16]            752\n",
      "├─PositionalEncoding: 1-2                     [1889, 30, 16]            --\n",
      "├─TransformerEncoder: 1-3                     [1889, 30, 16]            --\n",
      "│    └─ModuleList: 2-1                        --                        --\n",
      "│    │    └─TransformerEncoderLayer: 3-1      [1889, 30, 16]            68,752\n",
      "│    │    └─TransformerEncoderLayer: 3-2      [1889, 30, 16]            68,752\n",
      "│    │    └─TransformerEncoderLayer: 3-3      [1889, 30, 16]            68,752\n",
      "├─Linear: 1-4                                 [1889, 30, 47]            799\n",
      "├─Softmax: 1-5                                [1889, 30, 47]            --\n",
      "===============================================================================================\n",
      "Total params: 207,807\n",
      "Trainable params: 207,807\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 2.93\n",
      "===============================================================================================\n",
      "Input size (MB): 0.45\n",
      "Forward/backward pass size (MB): 28.56\n",
      "Params size (MB): 0.01\n",
      "Estimated Total Size (MB): 29.02\n",
      "===============================================================================================\n",
      "Epoch 10, Loss: 3.8313040733337402, Validation Loss: 3.828843593597412, Validation Accuracy: 0.10838618129491806\n",
      "Epoch 20, Loss: 3.8169338703155518, Validation Loss: 3.8133339881896973, Validation Accuracy: 0.20211414992809296\n",
      "Epoch 30, Loss: 3.8015975952148438, Validation Loss: 3.797322988510132, Validation Accuracy: 0.22057786583900452\n",
      "Epoch 40, Loss: 3.78617000579834, Validation Loss: 3.781463146209717, Validation Accuracy: 0.22438335418701172\n",
      "Epoch 50, Loss: 3.7684433460235596, Validation Loss: 3.7641332149505615, Validation Accuracy: 0.2259337455034256\n",
      "Epoch 60, Loss: 3.748091220855713, Validation Loss: 3.743718147277832, Validation Accuracy: 0.23784354329109192\n",
      "Epoch 70, Loss: 3.7259581089019775, Validation Loss: 3.722227096557617, Validation Accuracy: 0.24559548497200012\n",
      "Epoch 80, Loss: 3.704036235809326, Validation Loss: 3.699289321899414, Validation Accuracy: 0.262367844581604\n",
      "Epoch 90, Loss: 3.6842641830444336, Validation Loss: 3.6804161071777344, Validation Accuracy: 0.2647639214992523\n",
      "Epoch 100, Loss: 3.668250322341919, Validation Loss: 3.664825677871704, Validation Accuracy: 0.2682170569896698\n",
      "Total execution time: 20.54611301422119 seconds\n",
      "Predicted next character: 'n'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# In[0]\n",
    "import torch\n",
    "import time\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torchinfo    \n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "with open('/home/csalitre/school/ecgr-5106/intro-to-deeplearning/Datasets/sample_data.txt', 'r') as file:\n",
    "    text = file.read()\n",
    "\n",
    "\n",
    "# Creating character vocabulary\n",
    "chars = sorted(list(set(text)))\n",
    "ix_to_char = {i: ch for i, ch in enumerate(chars)}\n",
    "char_to_ix = {ch: i for i, ch in enumerate(chars)}\n",
    "\n",
    "# Preparing the dataset for sequence predictiona\n",
    "X = []\n",
    "y = []\n",
    "max_length = 30  # Maximum length of input sequences\n",
    "for i in range(len(text) - max_length - 1):\n",
    "    sequence = text[i:i + max_length]\n",
    "    label_sequence = text[i+1:i + max_length + 1]  # Shift by one for the next character sequence\n",
    "    X.append([char_to_ix[char] for char in sequence])\n",
    "    y.append([char_to_ix[char] for char in label_sequence])\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "# Splitting the dataset into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Converting data to PyTorch tensors\n",
    "X_train = torch.tensor(X_train, dtype=torch.long).to(device)\n",
    "y_train = torch.tensor(y_train, dtype=torch.long).to(device)\n",
    "X_val = torch.tensor(X_val, dtype=torch.long).to(device)\n",
    "y_val = torch.tensor(y_val, dtype=torch.long).to(device)\n",
    "\n",
    "# Positional Encoding\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.encoding = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n",
    "        self.encoding[:, 0::2] = torch.sin(position * div_term)\n",
    "        self.encoding[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.encoding = self.encoding.unsqueeze(0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.encoding = self.encoding.to(device)\n",
    "        return x + self.encoding[:, :x.size(1)].detach()\n",
    "\n",
    "# Defining the Transformer model\n",
    "class CharTransformer(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers, nhead):\n",
    "        super(CharTransformer, self).__init__()\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.pos_encoder = PositionalEncoding(hidden_size)\n",
    "        encoder_layers = nn.TransformerEncoderLayer(hidden_size, nhead, batch_first=True)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_layers)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.Softmax(dim=2)  # Softmax layer over the feature dimension\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        embedded = self.pos_encoder(embedded)\n",
    "        transformer_output = self.transformer_encoder(embedded)\n",
    "        output = self.fc(transformer_output)\n",
    "        return self.softmax(output)  # Apply softmax to the linear layer output\n",
    "\n",
    "# Hyperparameters\n",
    "hidden_size = 16\n",
    "num_layers = 3\n",
    "nhead = 2\n",
    "learning_rate = 0.001\n",
    "epochs = 100\n",
    "\n",
    "# Model, loss, and optimizer\n",
    "model = CharTransformer(len(chars), hidden_size, len(chars), num_layers, nhead).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "summary = torchinfo.summary(model, input_data=X_train)\n",
    "print(summary)\n",
    "\n",
    "# In[1]\n",
    "\n",
    "total_start_time = time.time()\n",
    "# Training the model\n",
    "for epoch in range(epochs):\n",
    "    start_time = time.time()\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(X_train)\n",
    "    loss = criterion(output.transpose(1, 2), y_train)  # Reshape output to match the CrossEntropyLoss expectations\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_output = model(X_val)\n",
    "        val_loss = criterion(val_output.transpose(1, 2), y_val)  # Same transpose for validation\n",
    "        _, predicted = torch.max(val_output, 2)  # Adjust dimension for prediction\n",
    "        val_accuracy = (predicted == y_val).float().mean()  # Calculate accuracy\n",
    "\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        end_time = time.time()\n",
    "        print(f'Epoch {epoch+1}, Loss: {loss.item()}, Validation Loss: {val_loss.item()}, Validation Accuracy: {val_accuracy.item()}')\n",
    "total_end_time = time.time()\n",
    "total_execution_time =  total_end_time - total_start_time\n",
    "print(f'Total execution time: {total_execution_time} seconds')\n",
    "\n",
    "# In[2]\n",
    "\n",
    "# Prediction function7\n",
    "def predict_next_char(model, char_to_ix, ix_to_char, initial_str):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        initial_input = torch.tensor([char_to_ix[c] for c in initial_str[-max_length:]], dtype=torch.long).unsqueeze(0).to(device)\n",
    "        prediction = model(initial_input)\n",
    "        #print(\"Prediction shape:\", prediction.shape)\n",
    "        last_timestep_pred = prediction.squeeze(0)[-1]\n",
    "        predicted_index = torch.argmax(last_timestep_pred, dim=0).item()\n",
    "        return ix_to_char[predicted_index]\n",
    "\n",
    "\n",
    "# Predicting the next character\n",
    "test_str = \"This is a simple example to demonstrate ho\"\n",
    "predicted_char = predict_next_char(model, char_to_ix, ix_to_char, test_str)\n",
    "print(f\"Predicted next character: '{predicted_char}'\")\n",
    "# %%\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
